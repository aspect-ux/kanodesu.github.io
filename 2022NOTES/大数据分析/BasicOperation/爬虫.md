#### 爬虫

##### 解码

使用python内置的`urllib.request`模块就可以获得网页字节码，乱码；通过对网页解码，就可以得到原码的字符串;`request`更推荐用；

###### **编码处理**

bytes - > decode - > `str(unicode)` - > encode - > bytes

已知unicode是可以识别的

##### 文本解析库Beautiful Soup

能够自动将输入文档转化成`unicode`,即字符串形式，编码为`utf-8`

以下为方法：

> #第一个参数是html文本字符串,第二个是解析器
>
> `BeautifulSoup(markup,"html.parser")`
>
> `html.parser = "lxml" | "xml" |"html5lib"`

解析器用来解析html文本，`BeautifulSoup`用来定位标签结点

可以使用**标准选择器**和**`css选择器`**

其中标准选择器有遍历文档树和搜索文档树

###### 标准选择器

*  遍历文档树

遍历，慢，只能匹配到与之相同的第一个标签；

text = """ <html></hmtl>     """

```python
soup.p.name  #获取名字
soup.p.string #获取内容
soup.p.attrs  #获取属性
soup.p['class']#获取属性值
```

* 搜索文档树

  `find(),find_all()方法`

  ```python
  find_all('ul')#返回列表
  
  #文本查询,只会返回内容
  print(find_all(text=""))
  
  ```

###### **CSS选择器**

* 基本语法

  `soup.select('.panel .panel-heading')`

  就是通过选择类，注意有空格

  `soup.select('ul li')`

  选择`ul`下面的li标签

  `soup.select('#list-2 .element')`

  #代表id，用来查找id为”list-2",类名为element的元素

  `soup.select('ul')[i]`

  也可以通过结点来选择

* more

  ```python
  #1
  for ul  in soup.select('ul'):
      print(ul.select('li'))
  #2
  for ul in soup.select('ul'):
      print(ul['id'])        #1
      print(ul.attrs['id'])  #2这两种方法都能获取属性如id
  #3
  for li in soup.select('li'):
      print('Get Text:',li.get_text())
      print('String:',li.string)
  ```

##### 正则表达式

* 准备

```python
#1
	#\d{3}  匹配三个数字
    #\d{3}\s+\d{3-8}   依次为三个数字+多个空格+ 3-8位数字
#2
	#[]表示范围
    #[0-9a-zA-Z\_]  表示一个数字或字母或下划线
#3
	#   ^表示必须以接在后面的字符或数字或...开头
    #   $相反
```

* `re`模块

```python
#  r''   使用后就不用担心转义的问题
import re
#1   查看是否匹配成功
test = "用户输入的字符串"
if(re.match(r'正则表达式’,test)):
            print("ok")
else print("error")

#2
m = re.match(r'',test)
m.group(0)    #默认原样返回
m.group(1)    #可以提取出第一个子串，以此类推（比如两组数字之间-）
m.groups()    #返回元组，提取所有子串
#3
'a b  c'.split(' ')#用指定字符分割，最后返回字符串列表，不能识别连续空格
 re.split(r'\s+','a b  c')#这个就可以，re模块的split更方便
            
#4
#贪婪匹配
#   可知分成了两组，若干个数字  +  以 0 结尾
# 由于贪婪匹配原则，匹配会一次性使第一组尽可能匹配得多使得第二组匹配不到0，            可以^(\d+?)(0*)$ 使得+是一个非贪婪性的
            
#5
#出于效率，(可能某一种正则表达式要使用很多次),可以预编译
re_tele = re.compile(r'')#
re_tele.match(str).group() #以后就不用重新编译这个正则了
            
#6
re.match() #从起始位置开始，也即只能匹配string本身
re.search() #返回第一个成功的子串
re.findall() # 返回所有匹配的子串
```



##### 应对反爬



* 伪装成浏览器

  ```python
  #requests: 添加headers
  
  #requests可以直接构造请求并发起。
  import requests
  url='https://www.douban.com'
  headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.186 Safari/537.36',
  }#可以通过chrome://version获取用户代理
  get_response = requests.get(url,headers=headers)
  print(get_response.text) 
  print(get_response.content)
  
  ```

* 使用代理IP和控制访问频率

  ```python
  Proxy_IP = 'http:\\111.72.126.111:8080'
  response = requests.get(url,proxies = Proxy_IP)
  time.sleep(2)
  ```

  

##### 实验

* `url`参数格式

  **<协议>：//<用户名>：<密码>@<主机域名或者`ip`地址>：<端口号>/<路径>；<参数>？<查询>#<片段>**

[湖北大学吧-百度贴吧--沙湖之滨，长江之畔，一环中最闪亮那枚星。--本吧是湖北大学师生校友在百度旗下的电子论坛，同时也欢迎有意报考湖北大学的高中毕业生、本科毕业生前来咨询。本吧诚迎五湖四海之士，同时亦希望 (baidu.com)](https://tieba.baidu.com/f?kw=湖北大学&ie=utf-8&tp=0&pn=49)

pn是page







代码分析

```python
# -*- coding: utf-8 -*-
"""
Created on Tue Apr 26 08:21:41 2022
@author: lenovo
"""
import requests
import time
from bs4 import BeautifulSoup #文本解析库
print("******\n 爬取中")
#time.sleep(2)
print("****\n")
import requests
#添加header,伪装成服务器访问
url='https://www.douban.com'
headers={'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.127 Safari/537.36',
    }
get_response = requests.get(url,headers=headers)
print(get_response.text) 
print(get_response.content)

#代理ip
Proxy_IP = { 'http':'http:\\111.72.126.111:808'}


#以上为反爬代码

f = open("D:/programming/condaProgram/爬虫/experiment.txt",'a+',encoding = 'utf-8')
get_time = time.strftime("%Y-%m-%d %H:%M:%S",time.localtime(time.time()))


#设置所爬取的网址
page = [0,50,155600]
for i in page:
    time.sleep(1)
    f.write("[时间："+get_time+"]\n[标题]湖北大学贴吧第"+str(i+1)+"页内容"+'\n')
    url = "https://tieba.baidu.com/f?kw=湖北大学&ie=utf-8&tp=0&pn="+ str(i)
    #url = "https://tieba.baidu.com/f?kw=%E6%B9%96%E5%8C%97%E5%A4%A7%E5%AD%A6&ie=utf-8&tp=0"
    html = requests.get(url)
    
    
    
#网页解析
    soup = BeautifulSoup(html.content, "lxml")
    print(soup.prettify())#打印格式
    
#从网页中提取想要的数据所在的节点
	#这里要注意，find_all返回的是一个列表
    all = soup.find_all("a",class_="j_th_tit")#获取标题查找标签中的class_="j_th_tit"，
    
    print(all)
    
 #将</a>作为分隔符去掉
	#str.split返回的仍然是一个列表，之后是一个字符串列表，内层中括号此时变成字符了
    ALL=str(all).split('</a>')
    ALL
#要注意将最后一个中括号pop出去，由于最后一个</a>在’】’之前，这时最后一个中括号被单独当成字符串。
    ALL.pop() 
    l=[]
    for s in ALL:
        #这里体现了为什么之前要pop，不然访问title后面的标题会出现outOfIndex,也可以通过限制循环次数
        l.append(s.split('title="')[0])
        #print(s.split('title="')[0])
    print(l)
   
    i=0
    for s in l:   
        #再次分片，由于分成两部分，用q接收第一部分，w接收第二部分
        q,w = s.split('">')
        i+=1
        f.write('【标题'+str(i)+'】：'+ q +'\n') 
                    
f.close()
```

